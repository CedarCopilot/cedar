---
title: 'Agent Backend Connection'
description: 'Overview of connecting AI agents to Cedar-OS'
---

# Agent Backend Connection

Before we get to the juicy interactions like speaking to your AI for it to do work inside your applications, we have to connect to the agent!

Cedar-OS has working configurations with every major backend. We support:

- **Mastra** - Full-featured agent framework with memory, tools, and knowledge base
- **AI SDK** - Vercel's unified AI SDK supporting multiple providers
- **OpenAI** - Direct OpenAI API integration
- **Anthropic** - Direct Anthropic API integration
- **Custom Backend** - Build your own integration
- **CedarRouter** (coming soon) - Our optimized routing solution

## How to choose your provider

- **You want a full-featured agent**: **Mastra** - When you need memory across sessions (conversation history), tool calls, knowledge base, and more complex agent capabilities.
- **Simplicity**: **AI SDK** - One interface, multiple providers. Perfect for getting started quickly.
- **You only have one API Key**: **Anthropic/OpenAI** - Direct integration with a single provider.
- **You already have a custom backend**: **Custom Backend** - (glad I could help)

## Initial Configuration

Choose your provider and configure it with the CedarCopilot component:

<CodeGroup>

```tsx AI SDK
import { CedarCopilot } from '@cedar-os/cedar';

function App() {
	return (
		// You don't need to put every model,
		// but if you try to use a model without a key it will fail
		<CedarCopilot
			llmProvider={{
				provider: 'ai-sdk',
				providers: {
					openai: {
						apiKey: process.env.OPENAI_API_KEY,
					},
					anthropic: {
						apiKey: process.env.ANTHROPIC_API_KEY,
					},
					google: {
						apiKey: process.env.GOOGLE_API_KEY,
					},
				},
			}}>
			<YourApp />
		</CedarCopilot>
	);
}
```

```tsx OpenAI
import { CedarCopilot } from '@cedar-os/cedar';

function App() {
	return (
		<CedarCopilot
			llmProvider={{
				provider: 'openai',
				apiKey: process.env.OPENAI_API_KEY,
			}}>
			<YourApp />
		</CedarCopilot>
	);
}
```

```tsx Anthropic
import { CedarCopilot } from '@cedar-os/cedar';

function App() {
	return (
		<CedarCopilot
			llmProvider={{
				provider: 'anthropic',
				apiKey: process.env.ANTHROPIC_API_KEY,
			}}>
			<YourApp />
		</CedarCopilot>
	);
}
```

```tsx Mastra
import { CedarCopilot } from '@cedar-os/cedar';

function App() {
	return (
		<CedarCopilot
			llmProvider={{
				provider: 'mastra',
				baseURL: 'http://localhost:3000/api', // Your Mastra backend URL
				apiKey: process.env.MASTRA_API_KEY, // Optional: only if your backend requires auth
			}}>
			<YourApp />
		</CedarCopilot>
	);
}
```

```tsx Custom Backend
import { CedarCopilot } from '@cedar-os/cedar';

function App() {
	return (
		<CedarCopilot
			llmProvider={{
				provider: 'custom',
				config: {
					baseURL: 'https://your-api.com',
					apiKey: 'your-api-key',
					// Any additional config your backend needs
					organizationId: 'org-123',
					projectId: 'project-456',
				},
			}}>
			<YourApp />
		</CedarCopilot>
	);
}
```

</CodeGroup>

## Detailed Provider Guides

For more advanced configuration and usage patterns:

- [Extending Mastra](/getting-started/agent-backend-connection/mastra) - Full agent framework with backend extension guides
- [Custom Backend](/getting-started/agent-backend-connection/custom) - Build your own integration through API

## Type Safety

Cedar-OS provides full TypeScript support. When you configure a provider, all methods are properly typed:

```tsx
import { useTypedAgentConnection } from '@cedar-os/cedar';

// Get a typed connection for your provider
const { callLLM, streamLLM } = useTypedAgentConnection('openai');

// TypeScript knows the exact parameters needed
const response = await callLLM({
	prompt: 'Hello, AI!',
	model: 'gpt-4o-mini', // Required for OpenAI
	temperature: 0.7,
});
```

## Next Steps

1. Choose your provider based on your needs
2. Follow the initialization code above
3. Start building your AI-powered application with Cedar-OS

For more advanced usage, see our guides on:

- [Chat Input](/getting-started/chat-input/overview) - Building rich chat interfaces
- [Messages](/getting-started/messages/overview) - Handling AI responses
- [State Access](/getting-started/state-access/overview) - Managing application state

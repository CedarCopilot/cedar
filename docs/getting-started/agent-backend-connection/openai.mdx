---
title: 'OpenAI (we recommend Mastra)'
description: 'Connecting Cedar-OS with OpenAI'
---

# OpenAI Integration

Direct OpenAI integration for when you only need OpenAI models. However, **we recommend using Mastra** for production applications as it provides additional features like memory, tools, and better error handling.

## Why We Recommend Mastra

While direct OpenAI integration works well, Mastra provides:

- Built-in conversation memory
- Tool calling capabilities
- Better error handling and retries
- Structured output support
- Knowledge base integration

Consider [Mastra](/getting-started/agent-backend-connection/mastra) if you need these features.

## Initial Configuration

For direct OpenAI integration:

```tsx
import { useCedarStore } from '@cedar-os/cedar';

const store = useCedarStore();

// Configure OpenAI provider
store.setProviderConfig({
	provider: 'openai',
	apiKey: process.env.OPENAI_API_KEY,
});

// Connect to the agent
await store.connect();
```

## Basic Usage

```tsx
import { useTypedAgentConnection } from '@cedar-os/cedar';

const { callLLM, streamLLM } = useTypedAgentConnection('openai');

// Non-streaming call
const response = await callLLM({
	model: 'gpt-4o-mini',
	prompt: 'Explain the theory of relativity',
	systemPrompt: 'You are a physics professor',
	temperature: 0.7,
	maxTokens: 1000,
});

console.log(response.content);
console.log(response.usage); // Token usage information
```

## Streaming Responses

For better user experience with long responses:

```tsx
const { streamLLM } = useTypedAgentConnection('openai');

const streamResponse = streamLLM(
	{
		model: 'gpt-4o',
		prompt: 'Write a comprehensive guide to machine learning',
		systemPrompt: 'You are an ML expert',
		temperature: 0.7,
	},
	(event) => {
		if (event.type === 'chunk') {
			// Update your UI with streaming content
			process.stdout.write(event.content);
		} else if (event.type === 'done') {
			console.log('\nStreaming complete');
		} else if (event.type === 'error') {
			console.error('Error:', event.error);
		}
	}
);

// Cancel if needed
// streamResponse.abort();
```

## Available Models

- `gpt-4o` - Most capable, multimodal
- `gpt-4o-mini` - Faster, more affordable
- `gpt-4-turbo` - Previous generation, still very capable
- `gpt-4` - Original GPT-4
- `gpt-3.5-turbo` - Fast, cost-effective
- `gpt-3.5-turbo-16k` - Larger context window

## Advanced Usage

### Function Calling

OpenAI supports function calling for structured interactions:

```tsx
const response = await callLLM({
	model: 'gpt-4o',
	prompt: "What's the weather in San Francisco?",
	functions: [
		{
			name: 'get_weather',
			description: 'Get the current weather in a location',
			parameters: {
				type: 'object',
				properties: {
					location: {
						type: 'string',
						description: 'The city and state, e.g. San Francisco, CA',
					},
					unit: {
						type: 'string',
						enum: ['celsius', 'fahrenheit'],
					},
				},
				required: ['location'],
			},
		},
	],
	function_call: 'auto',
});

// Check if the model wants to call a function
if (response.function_call) {
	console.log('Function to call:', response.function_call.name);
	console.log('Arguments:', response.function_call.arguments);
}
```

### JSON Mode

Force the model to respond with valid JSON:

```tsx
const response = await callLLM({
	model: 'gpt-4o-mini',
	prompt: 'List 5 programming languages with their key features',
	response_format: { type: 'json_object' },
	systemPrompt: 'You must respond with valid JSON',
});

const data = JSON.parse(response.content);
```

### Vision Capabilities

Use GPT-4o for image analysis:

```tsx
const response = await callLLM({
	model: 'gpt-4o',
	prompt: "What's in this image?",
	messages: [
		{
			role: 'user',
			content: [
				{ type: 'text', text: "What's in this image?" },
				{
					type: 'image_url',
					image_url: {
						url: 'https://example.com/image.jpg',
						detail: 'high',
					},
				},
			],
		},
	],
});
```

## Error Handling

Handle common OpenAI errors:

```tsx
try {
	const response = await callLLM({
		model: 'gpt-4o',
		prompt: 'Hello',
	});
} catch (error) {
	if (error.code === 'rate_limit_exceeded') {
		// Handle rate limiting
		console.log('Rate limited, waiting before retry...');
	} else if (error.code === 'invalid_api_key') {
		// Handle authentication errors
		console.error('Invalid API key');
	} else if (error.code === 'model_not_found') {
		// Handle model availability
		console.error('Model not available');
	} else {
		// Generic error handling
		console.error('OpenAI error:', error);
	}
}
```

## Best Practices

1. **Model Selection**:

   - Use `gpt-3.5-turbo` for simple, fast responses
   - Use `gpt-4o-mini` for balanced performance
   - Use `gpt-4o` for complex reasoning or vision tasks

2. **Cost Optimization**:

   ```tsx
   // Use cheaper models for simple tasks
   const classifyIntent = async (userInput: string) => {
   	return callLLM({
   		model: 'gpt-3.5-turbo',
   		prompt: `Classify this intent: ${userInput}`,
   		maxTokens: 50,
   		temperature: 0.3,
   	});
   };
   ```

3. **Context Management**:

   ```tsx
   // Manage conversation context manually
   const messages = [
   	{ role: 'system', content: 'You are a helpful assistant' },
   	{ role: 'user', content: 'Hello' },
   	{ role: 'assistant', content: 'Hi! How can I help?' },
   	{ role: 'user', content: 'Tell me about Cedar-OS' },
   ];

   const response = await callLLM({
   	model: 'gpt-4o-mini',
   	messages, // Pass full conversation history
   });
   ```

## Limitations

Direct OpenAI integration has some limitations:

1. **No built-in memory** - You must manage conversation history manually
2. **No automatic retries** - Implement your own retry logic
3. **Limited error handling** - Basic errors only
4. **No tool integration** - Function calling requires manual implementation

For these features, consider using [Mastra](/getting-started/agent-backend-connection/mastra) instead.

## Migration to Mastra

If you outgrow direct OpenAI integration:

```tsx
// Before (Direct OpenAI)
store.setProviderConfig({
	provider: 'openai',
	apiKey: 'sk-...',
});

// After (Mastra with OpenAI)
store.setProviderConfig({
	provider: 'mastra',
	baseURL: 'http://localhost:3000/api',
	apiKey: 'your-mastra-key',
});
// Configure Mastra backend to use OpenAI
```

## Next Steps

- Consider [Mastra](/getting-started/agent-backend-connection/mastra) for production use
- Explore [AI SDK](/getting-started/agent-backend-connection/ai-sdk) for multi-provider support
- Learn about [streaming patterns](/getting-started/messages/streaming)
